{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c94759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6017a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v2.5.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow v{}'.format(tf.__version__))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "hasGpu = False\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            hasGpu = True\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(hasGpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fab8b",
   "metadata": {},
   "source": [
    "# Data and Preprocessing\n",
    "\n",
    "This dataset consisted of 3 different indicators, EMA14 EMA50 ATR14, each with the previous 5 values at the time of trade entry.  This made for a total of 15 inputs with shape (15,) and containing 3 features.  The label was whether the trade was profitable (0 no, 1 yes).  This is by design so that the deep learning model acts as a `trade filter`.  The trade entry logic is simple `if-then` logic which sends the 3 indicators' previous 5 values as input.  The model takes this input and the prediction is whether the model expects the trade to be profitable (profitable includes anything breakeven or better).  If you receive a prediction of `1`, take the trade.  If you receive a prediction of `0`, you have 2 choices.  Either 1) don't take the trade 2) flip the trade to the opposite direction (go short instead of long, or long instead of short).  A prediction of `0` indicates the model expects the trade to be a losing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fa4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 3\n",
    "time_steps = 5\n",
    "windowSize = time_steps * features\n",
    "batchSize = 16\n",
    "trainingFileName = './TensorFlowFib50StratES 06-21Training.bin'\n",
    "#trainingFileName = './sonar.csv'\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a30902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build type array which matches NT8 indicators\n",
    "typeArray = []\n",
    "nameArray = []\n",
    "#for i in range (0, windowSize):\n",
    "#    typeArray.append((f'{i}', np.single))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    typeArray.append((f'ema14_{i}', np.single))\n",
    "    nameArray.append((f'ema14_{i}'))\n",
    "    \n",
    "for i in range(0, 5):\n",
    "    typeArray.append((f'ema50_{i}', np.single))\n",
    "    nameArray.append((f'ema50_{i}'))\n",
    "    \n",
    "for i in range(0, 5):\n",
    "    typeArray.append((f'atr14_{i}', np.single))\n",
    "    nameArray.append((f'atr14_{i}'))\n",
    "\n",
    "# typeArray.append(('isLong', np.bool)) # direction (1 = long, 0 = short)\n",
    "typeArray.append(('labels', np.int)) # is winner (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffa0011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ema14_0</th>\n",
       "      <th>ema14_1</th>\n",
       "      <th>ema14_2</th>\n",
       "      <th>ema14_3</th>\n",
       "      <th>ema14_4</th>\n",
       "      <th>ema50_0</th>\n",
       "      <th>ema50_1</th>\n",
       "      <th>ema50_2</th>\n",
       "      <th>ema50_3</th>\n",
       "      <th>ema50_4</th>\n",
       "      <th>atr14_0</th>\n",
       "      <th>atr14_1</th>\n",
       "      <th>atr14_2</th>\n",
       "      <th>atr14_3</th>\n",
       "      <th>atr14_4</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1277.457886</td>\n",
       "      <td>1277.430176</td>\n",
       "      <td>1277.439453</td>\n",
       "      <td>1277.380859</td>\n",
       "      <td>1277.230103</td>\n",
       "      <td>1276.714966</td>\n",
       "      <td>1276.735962</td>\n",
       "      <td>1276.765869</td>\n",
       "      <td>1276.775024</td>\n",
       "      <td>1276.754517</td>\n",
       "      <td>0.410450</td>\n",
       "      <td>0.416846</td>\n",
       "      <td>0.422786</td>\n",
       "      <td>0.446158</td>\n",
       "      <td>0.485718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1275.665649</td>\n",
       "      <td>1275.543457</td>\n",
       "      <td>1275.437744</td>\n",
       "      <td>1275.379395</td>\n",
       "      <td>1275.395386</td>\n",
       "      <td>1276.382568</td>\n",
       "      <td>1276.318604</td>\n",
       "      <td>1276.257080</td>\n",
       "      <td>1276.207764</td>\n",
       "      <td>1276.180054</td>\n",
       "      <td>0.445988</td>\n",
       "      <td>0.431989</td>\n",
       "      <td>0.401132</td>\n",
       "      <td>0.390337</td>\n",
       "      <td>0.416027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1277.730103</td>\n",
       "      <td>1277.832764</td>\n",
       "      <td>1277.988403</td>\n",
       "      <td>1278.023193</td>\n",
       "      <td>1278.020142</td>\n",
       "      <td>1277.028320</td>\n",
       "      <td>1277.086060</td>\n",
       "      <td>1277.161133</td>\n",
       "      <td>1277.203735</td>\n",
       "      <td>1277.234985</td>\n",
       "      <td>0.409185</td>\n",
       "      <td>0.415672</td>\n",
       "      <td>0.421696</td>\n",
       "      <td>0.445146</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1280.403687</td>\n",
       "      <td>1280.349854</td>\n",
       "      <td>1280.303223</td>\n",
       "      <td>1280.296143</td>\n",
       "      <td>1280.223267</td>\n",
       "      <td>1279.355225</td>\n",
       "      <td>1279.380493</td>\n",
       "      <td>1279.404785</td>\n",
       "      <td>1279.437988</td>\n",
       "      <td>1279.450195</td>\n",
       "      <td>0.414595</td>\n",
       "      <td>0.438552</td>\n",
       "      <td>0.407227</td>\n",
       "      <td>0.413854</td>\n",
       "      <td>0.420007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1278.876587</td>\n",
       "      <td>1278.826416</td>\n",
       "      <td>1278.716187</td>\n",
       "      <td>1278.654053</td>\n",
       "      <td>1278.666870</td>\n",
       "      <td>1279.300049</td>\n",
       "      <td>1279.268677</td>\n",
       "      <td>1279.218872</td>\n",
       "      <td>1279.180908</td>\n",
       "      <td>1279.164062</td>\n",
       "      <td>0.330037</td>\n",
       "      <td>0.306463</td>\n",
       "      <td>0.320287</td>\n",
       "      <td>0.315267</td>\n",
       "      <td>0.328462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69720</th>\n",
       "      <td>4081.594238</td>\n",
       "      <td>4082.048340</td>\n",
       "      <td>4082.408691</td>\n",
       "      <td>4082.554199</td>\n",
       "      <td>4082.346924</td>\n",
       "      <td>4075.612549</td>\n",
       "      <td>4075.980713</td>\n",
       "      <td>4076.324463</td>\n",
       "      <td>4076.605957</td>\n",
       "      <td>4076.778320</td>\n",
       "      <td>2.777451</td>\n",
       "      <td>2.632633</td>\n",
       "      <td>2.587445</td>\n",
       "      <td>2.509770</td>\n",
       "      <td>2.598358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69721</th>\n",
       "      <td>4096.187012</td>\n",
       "      <td>4096.495605</td>\n",
       "      <td>4096.529297</td>\n",
       "      <td>4096.692383</td>\n",
       "      <td>4097.033203</td>\n",
       "      <td>4096.894043</td>\n",
       "      <td>4096.957031</td>\n",
       "      <td>4096.948730</td>\n",
       "      <td>4096.979980</td>\n",
       "      <td>4097.069336</td>\n",
       "      <td>1.773560</td>\n",
       "      <td>1.789734</td>\n",
       "      <td>1.858324</td>\n",
       "      <td>1.814873</td>\n",
       "      <td>1.828096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69722</th>\n",
       "      <td>4097.964355</td>\n",
       "      <td>4098.335938</td>\n",
       "      <td>4098.824219</td>\n",
       "      <td>4099.147949</td>\n",
       "      <td>4097.594727</td>\n",
       "      <td>4097.461426</td>\n",
       "      <td>4097.590332</td>\n",
       "      <td>4097.763672</td>\n",
       "      <td>4097.900391</td>\n",
       "      <td>4097.492188</td>\n",
       "      <td>2.025478</td>\n",
       "      <td>2.220087</td>\n",
       "      <td>2.329367</td>\n",
       "      <td>2.270126</td>\n",
       "      <td>3.215117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69723</th>\n",
       "      <td>4099.832520</td>\n",
       "      <td>4099.588379</td>\n",
       "      <td>4099.309570</td>\n",
       "      <td>4099.268555</td>\n",
       "      <td>4098.332520</td>\n",
       "      <td>4095.795898</td>\n",
       "      <td>4095.882324</td>\n",
       "      <td>4095.945801</td>\n",
       "      <td>4096.065430</td>\n",
       "      <td>4095.915771</td>\n",
       "      <td>4.407694</td>\n",
       "      <td>4.307144</td>\n",
       "      <td>4.213777</td>\n",
       "      <td>4.127079</td>\n",
       "      <td>4.493001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69724</th>\n",
       "      <td>4107.764160</td>\n",
       "      <td>4107.662109</td>\n",
       "      <td>4107.640625</td>\n",
       "      <td>4107.588379</td>\n",
       "      <td>4107.476562</td>\n",
       "      <td>4102.066895</td>\n",
       "      <td>4102.260254</td>\n",
       "      <td>4102.465820</td>\n",
       "      <td>4102.653320</td>\n",
       "      <td>4102.813965</td>\n",
       "      <td>2.705889</td>\n",
       "      <td>2.619754</td>\n",
       "      <td>2.539771</td>\n",
       "      <td>2.465502</td>\n",
       "      <td>2.414395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69725 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ema14_0      ema14_1      ema14_2      ema14_3      ema14_4  \\\n",
       "0      1277.457886  1277.430176  1277.439453  1277.380859  1277.230103   \n",
       "1      1275.665649  1275.543457  1275.437744  1275.379395  1275.395386   \n",
       "2      1277.730103  1277.832764  1277.988403  1278.023193  1278.020142   \n",
       "3      1280.403687  1280.349854  1280.303223  1280.296143  1280.223267   \n",
       "4      1278.876587  1278.826416  1278.716187  1278.654053  1278.666870   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "69720  4081.594238  4082.048340  4082.408691  4082.554199  4082.346924   \n",
       "69721  4096.187012  4096.495605  4096.529297  4096.692383  4097.033203   \n",
       "69722  4097.964355  4098.335938  4098.824219  4099.147949  4097.594727   \n",
       "69723  4099.832520  4099.588379  4099.309570  4099.268555  4098.332520   \n",
       "69724  4107.764160  4107.662109  4107.640625  4107.588379  4107.476562   \n",
       "\n",
       "           ema50_0      ema50_1      ema50_2      ema50_3      ema50_4  \\\n",
       "0      1276.714966  1276.735962  1276.765869  1276.775024  1276.754517   \n",
       "1      1276.382568  1276.318604  1276.257080  1276.207764  1276.180054   \n",
       "2      1277.028320  1277.086060  1277.161133  1277.203735  1277.234985   \n",
       "3      1279.355225  1279.380493  1279.404785  1279.437988  1279.450195   \n",
       "4      1279.300049  1279.268677  1279.218872  1279.180908  1279.164062   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "69720  4075.612549  4075.980713  4076.324463  4076.605957  4076.778320   \n",
       "69721  4096.894043  4096.957031  4096.948730  4096.979980  4097.069336   \n",
       "69722  4097.461426  4097.590332  4097.763672  4097.900391  4097.492188   \n",
       "69723  4095.795898  4095.882324  4095.945801  4096.065430  4095.915771   \n",
       "69724  4102.066895  4102.260254  4102.465820  4102.653320  4102.813965   \n",
       "\n",
       "        atr14_0   atr14_1   atr14_2   atr14_3   atr14_4  labels  \n",
       "0      0.410450  0.416846  0.422786  0.446158  0.485718       0  \n",
       "1      0.445988  0.431989  0.401132  0.390337  0.416027       0  \n",
       "2      0.409185  0.415672  0.421696  0.445146  0.431207       0  \n",
       "3      0.414595  0.438552  0.407227  0.413854  0.420007       0  \n",
       "4      0.330037  0.306463  0.320287  0.315267  0.328462       1  \n",
       "...         ...       ...       ...       ...       ...     ...  \n",
       "69720  2.777451  2.632633  2.587445  2.509770  2.598358       0  \n",
       "69721  1.773560  1.789734  1.858324  1.814873  1.828096       1  \n",
       "69722  2.025478  2.220087  2.329367  2.270126  3.215117       0  \n",
       "69723  4.407694  4.307144  4.213777  4.127079  4.493001       1  \n",
       "69724  2.705889  2.619754  2.539771  2.465502  2.414395       0  \n",
       "\n",
       "[69725 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.fromfile(trainingFileName, dtype=(typeArray)))\n",
    "#df = pd.read_csv(trainingFileName, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6d8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supervised_dataset(data, isScale, isLabelEncode):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        X.append(data[i][:-1])\n",
    "        y.append(data[i][-1])\n",
    "        \n",
    "    if (isScale):\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "    if (isLabelEncode):\n",
    "        encoder = sklearn.preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        y = encoder.transform(y)\n",
    "        y = tf.keras.utils.to_categorical(y)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf02641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45462513 -1.45463814 -1.45465002 -1.45466034 -1.45470409 -1.45487758\n",
      " -1.45487165 -1.45486605 -1.45486074 -1.45486579 -0.64691622 -0.67441474\n",
      " -0.70018746 -0.72339685 -0.72899668] [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "dataset = df.values\n",
    "\n",
    "isLabelEncode = True\n",
    "X, y = create_supervised_dataset(dataset, isScale=True, isLabelEncode=isLabelEncode)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(X_train[i], y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756e46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57fdd61d",
   "metadata": {},
   "source": [
    "## Look at label distribution\n",
    "If the dataset is imbalanced, use `class_weights` parameter to better score the fit training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bacd548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = 0\n",
    "zeros = 0\n",
    "total = 1\n",
    "\n",
    "def analyze_dataset(dataset, labels):\n",
    "    train_ones = []\n",
    "    train_zeros = []\n",
    "    for i in range(0, len(labels)):\n",
    "        if (isLabelEncode == True):\n",
    "            if(np.argmax(labels[i]) == 1):\n",
    "                train_ones.append(1)\n",
    "            if(np.argmax(labels[i]) == 0):\n",
    "                train_zeros.append(0)\n",
    "        elif (isLabelEncode == False):\n",
    "            if(y_train[i] == 1):\n",
    "                train_ones.append(1)\n",
    "            if(y_train[i]== 0):\n",
    "                train_zeros.append(0)\n",
    "\n",
    "    ones = len(train_ones)\n",
    "    zeros = len(train_zeros)\n",
    "    total = ones + zeros\n",
    "    \n",
    "    ratio = np.maximum(ones, zeros) / np.minimum(ones, zeros)\n",
    "\n",
    "    print(f'ones: {ones} zeros: {zeros}')\n",
    "    print(f'if model predicts all zeros: {zeros/total:.2%}')\n",
    "    print(f'if model predicts all ones: {ones/total:.2%}')\n",
    "\n",
    "    # fit with weights\n",
    "    weights = {0:1, 1:1}\n",
    "\n",
    "    if (zeros > ones):\n",
    "        weights = {0:1, 1:ratio}\n",
    "        print(f'ratio 0s : 1s: {ratio}')\n",
    "    else:\n",
    "        weights = {0:ratio, 1:1}\n",
    "        print(f'ratio 1s : 0s: {ratio}')\n",
    "    pyplot.bar([0, 1], [zeros, ones])\n",
    "    \n",
    "    return zeros, ones, total, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "36d68f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 13105 zeros: 33610\n",
      "if model predicts all zeros: 71.95%\n",
      "if model predicts all ones: 28.05%\n",
      "ratio 0s : 1s: 2.5646699732926366\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zeros, ones, total, weights = analyze_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d8a5d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 6406 zeros: 16604\n",
      "if model predicts all zeros: 72.16%\n",
      "if model predicts all ones: 27.84%\n",
      "ratio 0s : 1s: 2.5919450515142053\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zeros, ones, total, weights = analyze_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f0f3f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones: 6406 zeros: 16604\n"
     ]
    }
   ],
   "source": [
    "print(f'ones: {ones} zeros: {zeros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107f4dc",
   "metadata": {},
   "source": [
    "## Define Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "44fe7913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model_accuracy(model):\n",
    "    correct = 0\n",
    "    num_predictions = 50\n",
    "    for i in range(0, num_predictions):\n",
    "        in_x = X_test[i].reshape(1, X_train.shape[1])\n",
    "        lbl = y_test[i]\n",
    "        yhat = model.predict(in_x)\n",
    "        if (isLabelEncode):\n",
    "            pred = np.argmax(yhat)\n",
    "            actual = np.argmax(y_test[i])\n",
    "            print(f'pred: [{yhat[0][0]:.3} {yhat[0][1]:.3}] actual: {y_test[i]} {\"|||\":10} pred: {pred} actual: {actual}')\n",
    "            \n",
    "            if (pred == actual):\n",
    "                correct += 1\n",
    "        else:\n",
    "            print(f'pred: {yhat[0]:.4} actual: {y_test[i]}')\n",
    "            \n",
    "    print(f'correct: {correct / num_predictions:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "44760fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    yhat = model.predict(X_test)\n",
    "    # evaluate the ROC AUC of the predictions\n",
    "    score = sklearn.metrics.roc_auc_score(y_test, yhat)\n",
    "    print(f'ROC AUC: {score:.3%}')\n",
    "    test_model_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b373f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network model\n",
    "def simple_model(x_input, outClasses):\n",
    "    # define model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # define first hidden layer and visible layer\n",
    "    model.add(tf.keras.layers.Dense(10, input_dim=x_input.shape[1], activation='relu',  kernel_initializer='he_uniform'))\n",
    "    # define output layer\n",
    "    #model.add(tf.keras.layers.Dense(outClasses, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(outClasses, activation='softmax'))\n",
    "    # define loss and optimizer\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e51737c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_out_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209696ec",
   "metadata": {},
   "source": [
    "### Test vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(X_train, num_out_classes)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3f1dea32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1460/1460 - 6s - loss: 0.6245 - accuracy: 0.6936 - val_loss: 0.5930 - val_accuracy: 0.7207\n",
      "Epoch 2/10\n",
      "1460/1460 - 5s - loss: 0.5933 - accuracy: 0.7191 - val_loss: 0.5897 - val_accuracy: 0.7215\n",
      "Epoch 3/10\n",
      "1460/1460 - 5s - loss: 0.5918 - accuracy: 0.7191 - val_loss: 0.5884 - val_accuracy: 0.7215\n",
      "Epoch 4/10\n",
      "1460/1460 - 5s - loss: 0.5913 - accuracy: 0.7195 - val_loss: 0.5882 - val_accuracy: 0.7216\n",
      "Epoch 5/10\n",
      "1460/1460 - 6s - loss: 0.5911 - accuracy: 0.7194 - val_loss: 0.5881 - val_accuracy: 0.7215\n",
      "Epoch 6/10\n",
      "1460/1460 - 5s - loss: 0.5910 - accuracy: 0.7194 - val_loss: 0.5879 - val_accuracy: 0.7215\n",
      "Epoch 7/10\n",
      "1460/1460 - 5s - loss: 0.5909 - accuracy: 0.7193 - val_loss: 0.5881 - val_accuracy: 0.7215\n",
      "Epoch 8/10\n",
      "1460/1460 - 5s - loss: 0.5908 - accuracy: 0.7195 - val_loss: 0.5878 - val_accuracy: 0.7215\n",
      "Epoch 9/10\n",
      "1460/1460 - 5s - loss: 0.5908 - accuracy: 0.7195 - val_loss: 0.5879 - val_accuracy: 0.7215\n",
      "Epoch 10/10\n",
      "1460/1460 - 5s - loss: 0.5907 - accuracy: 0.7193 - val_loss: 0.5877 - val_accuracy: 0.7215\n",
      "WARNING:tensorflow:5 out of the last 1548 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002601E390940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "ROC AUC: 55.525%\n",
      "pred: [0.8109334  0.18906665] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.92685616 0.07314387] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8464643  0.15353578] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.72897273 0.27102724] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.87448525 0.12551475] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.9009482  0.09905174] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.91422874 0.08577126] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.81315106 0.18684895] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.80166084 0.19833918] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.88320047 0.11679957] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8740631  0.12593694] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7487728  0.25122717] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8725472  0.12745278] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.90349257 0.09650743] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.85786265 0.14213742] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.9100776  0.08992248] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.83743423 0.16256574] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8318625  0.16813746] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.82252985 0.17747016] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.91836816 0.0816318 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8086151  0.19138491] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8865503  0.11344969] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.78833115 0.21166882] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8240833  0.17591669] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8983621 0.1016379] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.91793984 0.08206011] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.90075976 0.09924027] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.89461213 0.10538787] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.92320925 0.07679079] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.9115886  0.08841139] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8999988  0.10000124] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8420821  0.15791787] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.81107396 0.18892606] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8633451  0.13665488] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8453345  0.15466553] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8918773  0.10812267] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8159581 0.1840419] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.92186636 0.07813356] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.85403764 0.14596231] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.91105    0.08895001] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7988126  0.20118733] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.93215466 0.06784532] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.90978014 0.0902198 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.89171195 0.10828809] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.81930584 0.18069412] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.8839773  0.11602271] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8622941 0.1377059] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.924033   0.07596703] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.8349455  0.16505454] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.9099786  0.09002133] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 74.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76c3fc",
   "metadata": {},
   "source": [
    "74% correct!!! At first glance, this looks fantastic.  However, if the training dataset is used for comparison, a model which predicts all zeros must do better than 71.95%.  The numbers are slightly different for the test dataset but the idea is the same.  Due to the stochastic nature of deep learning, this approximately 2% advantage should be met with significant skepticism.  Given the fact that the dataset is imbalanced, the `class_weight` parameter should be utilized at a minimum.  SMOTE or other sampling techniques would likely be a more robust way to evaluate this model.\n",
    "\n",
    "In classification problems, accuracy on tells part of the story (often a less important part).  ROC AUC and F1 score are often a much better indicator of the ability of the model.  This super simple model only achieves a ROC AUC of 55% which is nothing stellar and is not likely to have reliable prediction skill.\n",
    "\n",
    "The final indication that this model is essentially garbage is the predictions are always 0.  There could be several reasons for this:\n",
    " - Massive overfitting (most likely in this case)\n",
    " - Too aggressive of a learning rate\n",
    " - Too complex of a model (model just memorizes the dataset)\n",
    " - Imbalanced dataset\n",
    " \n",
    "This model's predictions vary only a slight amount and always are heavily weighted to predict zero.  The fact that the prediction probabilities do change means that the model is sound, but is just overall terrible.  This is to be expected with a single Dense layer with only 10 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e1de1",
   "metadata": {},
   "source": [
    "### Test properly class weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851da59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jason\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/10\n",
      "1460/1460 - 6s - loss: 1.0014 - accuracy: 0.5640 - val_loss: 0.6877 - val_accuracy: 0.5658\n",
      "Epoch 2/10\n",
      "1460/1460 - 6s - loss: 0.9923 - accuracy: 0.5568 - val_loss: 0.6925 - val_accuracy: 0.5341\n",
      "Epoch 3/10\n",
      "1460/1460 - 6s - loss: 0.9922 - accuracy: 0.5464 - val_loss: 0.6834 - val_accuracy: 0.5651\n",
      "Epoch 4/10\n",
      "1460/1460 - 6s - loss: 0.9921 - accuracy: 0.5525 - val_loss: 0.6977 - val_accuracy: 0.5229\n",
      "Epoch 5/10\n",
      "1460/1460 - 6s - loss: 0.9920 - accuracy: 0.5440 - val_loss: 0.6819 - val_accuracy: 0.5682\n",
      "Epoch 6/10\n",
      "1460/1460 - 6s - loss: 0.9920 - accuracy: 0.5520 - val_loss: 0.6906 - val_accuracy: 0.5407\n",
      "Epoch 7/10\n",
      "1460/1460 - 6s - loss: 0.9919 - accuracy: 0.5496 - val_loss: 0.7003 - val_accuracy: 0.5161\n",
      "Epoch 8/10\n",
      "1460/1460 - 6s - loss: 0.9918 - accuracy: 0.5455 - val_loss: 0.6870 - val_accuracy: 0.5507\n",
      "Epoch 9/10\n",
      "1460/1460 - 6s - loss: 0.9918 - accuracy: 0.5516 - val_loss: 0.6904 - val_accuracy: 0.5445\n",
      "Epoch 10/10\n",
      "1460/1460 - 6s - loss: 0.9917 - accuracy: 0.5470 - val_loss: 0.6946 - val_accuracy: 0.5305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25fd63efe80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit with weights\n",
    "weights = {0:1, 1:1}\n",
    "\n",
    "if (zeros > ones):\n",
    "    #weights = {0:ratio, 1:1}\n",
    "    weights = {0:1, 1:ratio}\n",
    "else:\n",
    "    #weights = {0:1, 1:ratio}\n",
    "    weights = {0:ratio, 1:1}\n",
    "\n",
    "model = define_model(X_train, num_out_classes)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c603c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 55.630%\n",
      "pred: [0.4229445  0.57705545] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.62145513 0.3785449 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.41913956 0.5808605 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.35279363 0.6472064 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.51803356 0.48196644] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5645756  0.43542436] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.60645044 0.3935496 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4228555 0.5771445] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.40475908 0.59524095] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5370234 0.4629766] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4626595  0.53734046] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.36453125 0.6354687 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4810305 0.5189695] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.52447855 0.47552142] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.48799127 0.51200867] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.6189828  0.38101727] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.46265942 0.5373406 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.3696596 0.6303404] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.4549318  0.54506826] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.56872433 0.43127564] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.42283204 0.577168  ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4902302  0.50976974] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.39611283 0.60388714] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.37092006 0.62908   ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.6232337  0.37676632] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.60090053 0.39909947] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.56586015 0.43413985] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.56251806 0.43748197] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5927994  0.40720057] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.55686486 0.44313508] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.57823503 0.42176494] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.40892124 0.59107876] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.40191564 0.59808433] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5015131  0.49848694] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.4230733 0.5769267] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.51039076 0.48960927] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4229445  0.57705545] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.63080823 0.36919174] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4097174 0.5902826] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5798627  0.42013732] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.39266393 0.6073361 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.6464537  0.35354632] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.57425594 0.4257441 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5875082  0.41249183] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.43179604 0.568204  ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4801652  0.51983476] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.4410875 0.5589125] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5798764  0.42012355] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.3890815 0.6109185] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.55392045 0.44607955] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 50.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8155f5",
   "metadata": {},
   "source": [
    "This model with adjusted class weights gives a much more accurate representation of this model's true abilities.  When digging into the predictions, they do vary between predicting each of the 2 classes (0 and 1 - loser/winner).  however, the model's confidence is not great.  If I were to use a model in actual trading, I would want the model to be much, much more certain of its predicction 90% or better.  That would correspond to [0.9 0.1] for a zero (predict a loser, so don't trade) or [0.1 0.9] for a one (predicting a winner, take the trade).\n",
    "\n",
    "The ROC AUC is only 55.6% which is still terrible.  I would want something well over 80% before I even considered forward testing it on a demo account.  The accuracy of 50% is much more representative of this model's true ability.  It has no skill due to any one or more of the following:\n",
    "\n",
    " - The features do not possess any predictive information (most likely the case).\n",
    " - The model is not of sufficient complexity to extract predictive relationships in the data (also very likely).\n",
    " - Class weighting was not sufficient to overcome the dataset imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925655e",
   "metadata": {},
   "source": [
    "# Try more advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f3d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_deeper(x_input, outClasses):\n",
    "    # define model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # define first hidden layer and visible layer\n",
    "    model.add(tf.keras.layers.Dense(50, input_dim=x_input.shape[1], activation='relu',  kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu',  kernel_initializer='he_uniform'))\n",
    "    # define output layer\n",
    "    #model.add(tf.keras.layers.Dense(outClasses, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(outClasses, activation='softmax'))\n",
    "    # define loss and optimizer\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bc41d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(x_input, outClasses):\n",
    "    # let's use the functional api to build this one\n",
    "    visible = tf.keras.layers.Input(shape=(x_input.shape[1]))\n",
    "    \n",
    "    #CNN requires 3D input in the shape [samples, time_steps, features]\n",
    "    print(f'time_steps: {time_steps} featuers: {features}')\n",
    "    reshape = tf.keras.layers.Reshape(target_shape=(time_steps,features))(visible)\n",
    "    \n",
    "    cnn = tf.keras.layers.Conv1D(32, kernel_size=2, strides=1, activation=tf.keras.activations.relu, padding='same')(reshape)\n",
    "    cnn = tf.keras.layers.Conv1D(32, kernel_size=2, strides=1, activation=tf.keras.activations.relu, padding='same')(reshape)\n",
    "    pool = tf.keras.layers.MaxPooling1D()(cnn)\n",
    "    flat = tf.keras.layers.Flatten()(pool)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(outClasses, activation=tf.keras.activations.sigmoid)(flat)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d2d2a835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46715, 15)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22b51bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 5 featuers: 3\n"
     ]
    }
   ],
   "source": [
    "model = CNN(X_train, num_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fed206a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 15)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 5, 3)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5, 32)             224       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 354\n",
      "Trainable params: 354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "13470eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw prediction: [0.54786   0.7192231] predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# test a prediction to verify model design\n",
    "x_input = X_test[0]\n",
    "x_input = x_input.reshape(1,X_test.shape[1])\n",
    "yhat = model.predict(x_input)\n",
    "print(f'raw prediction: {yhat[0]} predicted label: {np.argmax(yhat[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78a66d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1460/1460 - 5s - loss: 0.6076 - accuracy: 0.7123 - val_loss: 0.5921 - val_accuracy: 0.7215\n",
      "Epoch 2/10\n",
      "1460/1460 - 4s - loss: 0.5926 - accuracy: 0.7193 - val_loss: 0.5892 - val_accuracy: 0.7215\n",
      "Epoch 3/10\n",
      "1460/1460 - 4s - loss: 0.5912 - accuracy: 0.7194 - val_loss: 0.5886 - val_accuracy: 0.7214\n",
      "Epoch 4/10\n",
      "1460/1460 - 4s - loss: 0.5907 - accuracy: 0.7193 - val_loss: 0.5888 - val_accuracy: 0.7216\n",
      "Epoch 5/10\n",
      "1460/1460 - 4s - loss: 0.5906 - accuracy: 0.7194 - val_loss: 0.5880 - val_accuracy: 0.7215\n",
      "Epoch 6/10\n",
      "1460/1460 - 4s - loss: 0.5905 - accuracy: 0.7195 - val_loss: 0.5881 - val_accuracy: 0.7216\n",
      "Epoch 7/10\n",
      "1460/1460 - 4s - loss: 0.5905 - accuracy: 0.7194 - val_loss: 0.5880 - val_accuracy: 0.7215\n",
      "Epoch 8/10\n",
      "1460/1460 - 4s - loss: 0.5904 - accuracy: 0.7194 - val_loss: 0.5879 - val_accuracy: 0.7214\n",
      "Epoch 9/10\n",
      "1460/1460 - 4s - loss: 0.5905 - accuracy: 0.7194 - val_loss: 0.5879 - val_accuracy: 0.7215\n",
      "Epoch 10/10\n",
      "1460/1460 - 4s - loss: 0.5904 - accuracy: 0.7195 - val_loss: 0.5880 - val_accuracy: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2603d738a00>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc6b41b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 55.441%\n",
      "pred: [0.6974353 0.2989538] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7678357  0.23545521] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.714232  0.2913712] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.64331245 0.35564148] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.735261   0.25863504] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7526814  0.24416482] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7721209  0.22969149] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.6984429  0.30058646] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.67184204 0.32218963] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7399283  0.25608483] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.73074716 0.27386573] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.6723269  0.33441693] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7224862  0.27406755] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.74743307 0.25064293] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7197752 0.2783008] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7737068  0.22520329] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.713358  0.2843269] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.65463036 0.34383407] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.69722974 0.30220726] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.75372    0.24379264] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.70869756 0.2932697 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7375809 0.263189 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.677525   0.32779288] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.6393771 0.3476252] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7689209  0.22951023] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7619318  0.23546928] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7533504  0.25798976] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.75449455 0.24246295] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.75307614 0.24450664] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7465707  0.25298956] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7517095  0.24833955] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7079553 0.296698 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.6701677  0.33675352] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.72338617 0.27597848] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7126751  0.29198325] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.736607   0.26418078] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.68978345 0.30058768] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7720018 0.2267817] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.69737965 0.30833668] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.75824934 0.2371163 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.6712237 0.3309511] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.77774847 0.22357035] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7503163  0.24785459] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7605153  0.23415771] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.71720594 0.28490582] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.7377541  0.26299724] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.72568375 0.2748329 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7591703  0.23713149] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7008908  0.30148855] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.7490474  0.24927677] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 74.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513dee",
   "metadata": {},
   "source": [
    "Even with a significantly more advanced model architecture, the non class weighted model performs virtually identical to the extremely simple single Dense layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7dd16bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 5 featuers: 3\n",
      "Epoch 1/50\n",
      "1460/1460 - 4s - loss: 0.9992 - accuracy: 0.5708 - val_loss: 0.6964 - val_accuracy: 0.5571\n",
      "Epoch 2/50\n",
      "1460/1460 - 4s - loss: 0.9934 - accuracy: 0.5837 - val_loss: 0.6860 - val_accuracy: 0.6011\n",
      "Epoch 3/50\n",
      "1460/1460 - 4s - loss: 0.9928 - accuracy: 0.5771 - val_loss: 0.6851 - val_accuracy: 0.5926\n",
      "Epoch 4/50\n",
      "1460/1460 - 4s - loss: 0.9925 - accuracy: 0.5745 - val_loss: 0.6934 - val_accuracy: 0.5525\n",
      "Epoch 5/50\n",
      "1460/1460 - 4s - loss: 0.9923 - accuracy: 0.5646 - val_loss: 0.6820 - val_accuracy: 0.5929\n",
      "Epoch 6/50\n",
      "1460/1460 - 4s - loss: 0.9922 - accuracy: 0.5663 - val_loss: 0.6891 - val_accuracy: 0.5621\n",
      "Epoch 7/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5640 - val_loss: 0.7006 - val_accuracy: 0.5155\n",
      "Epoch 8/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5534 - val_loss: 0.6803 - val_accuracy: 0.5947\n",
      "Epoch 9/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5628 - val_loss: 0.6817 - val_accuracy: 0.5895\n",
      "Epoch 10/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5658 - val_loss: 0.6962 - val_accuracy: 0.5349\n",
      "Epoch 11/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5585 - val_loss: 0.6885 - val_accuracy: 0.5637\n",
      "Epoch 12/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5620 - val_loss: 0.6904 - val_accuracy: 0.5575\n",
      "Epoch 13/50\n",
      "1460/1460 - 4s - loss: 0.9921 - accuracy: 0.5631 - val_loss: 0.6983 - val_accuracy: 0.5252\n",
      "Epoch 14/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5586 - val_loss: 0.6911 - val_accuracy: 0.5569\n",
      "Epoch 15/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5594 - val_loss: 0.6875 - val_accuracy: 0.5694\n",
      "Epoch 16/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5658 - val_loss: 0.6879 - val_accuracy: 0.5660\n",
      "Epoch 17/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5628 - val_loss: 0.6953 - val_accuracy: 0.5376\n",
      "Epoch 18/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5559 - val_loss: 0.6860 - val_accuracy: 0.5769\n",
      "Epoch 19/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5607 - val_loss: 0.6839 - val_accuracy: 0.5836\n",
      "Epoch 20/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5613 - val_loss: 0.6806 - val_accuracy: 0.5922\n",
      "Epoch 21/50\n",
      "1460/1460 - 4s - loss: 0.9920 - accuracy: 0.5608 - val_loss: 0.6836 - val_accuracy: 0.5832\n",
      "Epoch 22/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5620 - val_loss: 0.6882 - val_accuracy: 0.5662\n",
      "Epoch 23/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5622 - val_loss: 0.6922 - val_accuracy: 0.5587\n",
      "Epoch 24/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5645 - val_loss: 0.6932 - val_accuracy: 0.5469\n",
      "Epoch 25/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5620 - val_loss: 0.6967 - val_accuracy: 0.5300\n",
      "Epoch 26/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5589 - val_loss: 0.6912 - val_accuracy: 0.5554\n",
      "Epoch 27/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5607 - val_loss: 0.6843 - val_accuracy: 0.5782\n",
      "Epoch 28/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5612 - val_loss: 0.6931 - val_accuracy: 0.5508\n",
      "Epoch 29/50\n",
      "1460/1460 - 4s - loss: 0.9919 - accuracy: 0.5607 - val_loss: 0.6905 - val_accuracy: 0.5585\n",
      "Epoch 30/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5610 - val_loss: 0.6909 - val_accuracy: 0.5583\n",
      "Epoch 31/50\n",
      "1460/1460 - 4s - loss: 0.9917 - accuracy: 0.5568 - val_loss: 0.6813 - val_accuracy: 0.5914\n",
      "Epoch 32/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5633 - val_loss: 0.6804 - val_accuracy: 0.5908\n",
      "Epoch 33/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5625 - val_loss: 0.6915 - val_accuracy: 0.5553\n",
      "Epoch 34/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5611 - val_loss: 0.6901 - val_accuracy: 0.5618\n",
      "Epoch 35/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5625 - val_loss: 0.6881 - val_accuracy: 0.5682\n",
      "Epoch 36/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5641 - val_loss: 0.6924 - val_accuracy: 0.5507\n",
      "Epoch 37/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5625 - val_loss: 0.6890 - val_accuracy: 0.5648\n",
      "Epoch 38/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5590 - val_loss: 0.6902 - val_accuracy: 0.5621\n",
      "Epoch 39/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5655 - val_loss: 0.6941 - val_accuracy: 0.5455\n",
      "Epoch 40/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5608 - val_loss: 0.6907 - val_accuracy: 0.5622\n",
      "Epoch 41/50\n",
      "1460/1460 - 4s - loss: 0.9917 - accuracy: 0.5632 - val_loss: 0.6925 - val_accuracy: 0.5535\n",
      "Epoch 42/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5630 - val_loss: 0.6916 - val_accuracy: 0.5561\n",
      "Epoch 43/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5617 - val_loss: 0.6874 - val_accuracy: 0.5671\n",
      "Epoch 44/50\n",
      "1460/1460 - 4s - loss: 0.9916 - accuracy: 0.5656 - val_loss: 0.6957 - val_accuracy: 0.5408\n",
      "Epoch 45/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5604 - val_loss: 0.6883 - val_accuracy: 0.5689\n",
      "Epoch 46/50\n",
      "1460/1460 - 4s - loss: 0.9918 - accuracy: 0.5637 - val_loss: 0.6921 - val_accuracy: 0.5516\n",
      "Epoch 47/50\n",
      "1460/1460 - 4s - loss: 0.9916 - accuracy: 0.5615 - val_loss: 0.6878 - val_accuracy: 0.5678\n",
      "Epoch 48/50\n",
      "1460/1460 - 4s - loss: 0.9917 - accuracy: 0.5630 - val_loss: 0.6881 - val_accuracy: 0.5669\n",
      "Epoch 49/50\n",
      "1460/1460 - 4s - loss: 0.9917 - accuracy: 0.5629 - val_loss: 0.6902 - val_accuracy: 0.5630\n",
      "Epoch 50/50\n",
      "1460/1460 - 4s - loss: 0.9916 - accuracy: 0.5625 - val_loss: 0.6891 - val_accuracy: 0.5671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25fb927d0d0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(X_train, num_out_classes)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bd4bf82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 55.387%\n",
      "pred: [0.47852793 0.5147453 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5710444  0.43485552] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.47914904 0.5174108 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.40179121 0.5961109 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5164035  0.47852772] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.540234  0.4625015] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5703057 0.4350108] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.47672233 0.51606303] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4503415 0.5531159] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.51860744 0.47851732] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49212158 0.50521904] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4330365 0.5738932] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.50752896 0.48682916] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.52090144 0.4792694 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.50736296 0.49000847] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5691153 0.4347376] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4787451 0.5190347] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.4195503  0.57505566] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.48860762 0.5060218 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.53848857 0.4618188 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.47475266 0.51480764] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.507899   0.49049774] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.44359648 0.5591546 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.40726426 0.59184694] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5595862 0.4415319] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5571279  0.44489738] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.54637057 0.4623985 ] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.5452998 0.4560325] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.54640114 0.45296493] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5324045  0.46581045] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.52861995 0.46961737] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.46536365 0.5365222 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.44049847 0.57104707] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50806856 0.49067423] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.47562706 0.52257454] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5133055  0.48363653] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.4771603 0.5151774] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.56061506 0.4404282 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.45389432 0.5449355 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5465767  0.45301244] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.44092575 0.57084984] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5741169  0.43016967] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5447071  0.45780948] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5481634  0.45359194] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5006906 0.505987 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.5037053  0.49557933] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49042937 0.50749224] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5451641  0.45660827] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.45492905 0.5477283 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5268214  0.47178686] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 58.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b19669",
   "metadata": {},
   "source": [
    "The accuracy of this CNN-based model is fairly noticeable improvement over the very simple model with a 8% improvement.  However, the ROC AUC is virtually identical which means this model is essentially no better with the quality of its predictions.  If we carefully analyze the `take trade (1)` versus `don't take trade (0)`, there are quite a significant false negatives.  The high ratio of false negatives combined with the low confidence of the predicted label could mean a few things:\n",
    "\n",
    " - This model need more training.  Even with 50 epochs, the model simply became overfitted with no improvement in skill.\n",
    " - The features do not have strong predictive information.  This is looking much more of a certainty now.\n",
    " \n",
    "Let's try other architectures to see if there is any salvaging these indicator based inputs\n",
    " - Encoder decoder\n",
    " - Recurrent Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be466f2",
   "metadata": {},
   "source": [
    "## Enccoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0eeeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(x_input, outClasses):\n",
    "    # let's use the functional api to build this one\n",
    "    visible = tf.keras.layers.Input(shape=(x_input.shape[1]))    \n",
    "    reshape = tf.keras.layers.Reshape(target_shape=(time_steps,features))(visible)\n",
    "    \n",
    "    enc = tf.compat.v1.keras.layers.LSTM(20, activation='relu')(reshape)\n",
    "        \n",
    "    bottleneck = tf.keras.layers.RepeatVector(outClasses)(enc)\n",
    "    dec = tf.compat.v1.keras.layers.LSTM(20, activation='relu', return_sequences=True)(bottleneck)\n",
    "    \n",
    "    dec  = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(time_steps, activation='relu'))(dec)\n",
    "    dec = tf.keras.layers.Flatten()(dec)\n",
    "    output = tf.keras.layers.Dense(outClasses, activation='softmax')(dec)    \n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2da5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder_decoder(X_train, num_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7ec0f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 15)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_27 (Reshape)         (None, 5, 3)              0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 20)                1920      \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 2, 20)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 2, 20)             3280      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 2, 5)              105       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 5,327\n",
      "Trainable params: 5,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d8f4be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw prediction: [0.49987254 0.5001275 ] predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# test a prediction to verify model design\n",
    "x_input = X_test[0]\n",
    "x_input = x_input.reshape(1,X_test.shape[1])\n",
    "yhat = model.predict(x_input)\n",
    "print(f'raw prediction: {yhat[0]} predicted label: {np.argmax(yhat[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ab8f1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1460/1460 - 77s - loss: 0.9971 - accuracy: 0.4336 - val_loss: 0.6937 - val_accuracy: 0.4432\n",
      "Epoch 2/10\n",
      "1460/1460 - 75s - loss: 0.9971 - accuracy: 0.4529 - val_loss: 0.6937 - val_accuracy: 0.4615\n",
      "Epoch 3/10\n",
      "1460/1460 - 75s - loss: 0.9971 - accuracy: 0.4710 - val_loss: 0.6937 - val_accuracy: 0.4768\n",
      "Epoch 4/10\n",
      "1460/1460 - 75s - loss: 0.9970 - accuracy: 0.4849 - val_loss: 0.6936 - val_accuracy: 0.4912\n",
      "Epoch 5/10\n",
      "1460/1460 - 76s - loss: 0.9970 - accuracy: 0.4941 - val_loss: 0.6936 - val_accuracy: 0.5024\n",
      "Epoch 6/10\n",
      "1460/1460 - 77s - loss: 0.9970 - accuracy: 0.5073 - val_loss: 0.6936 - val_accuracy: 0.5117\n",
      "Epoch 7/10\n",
      "1460/1460 - 76s - loss: 0.9970 - accuracy: 0.5153 - val_loss: 0.6936 - val_accuracy: 0.5196\n",
      "Epoch 8/10\n",
      "1460/1460 - 77s - loss: 0.9970 - accuracy: 0.5226 - val_loss: 0.6936 - val_accuracy: 0.5245\n",
      "Epoch 9/10\n",
      "1460/1460 - 76s - loss: 0.9970 - accuracy: 0.5284 - val_loss: 0.6936 - val_accuracy: 0.5297\n",
      "Epoch 10/10\n",
      "1460/1460 - 77s - loss: 0.9969 - accuracy: 0.5334 - val_loss: 0.6936 - val_accuracy: 0.5335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x260ba2736a0>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "acc853f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 54.801%\n",
      "pred: [0.49445245 0.5055475 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5022087  0.49779126] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49965802 0.500342  ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4906386 0.5093614] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.49976245 0.5002375 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.49927184 0.5007282 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50061977 0.4993802 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49412128 0.50587875] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.49528664 0.5047133 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.49858567 0.5014143 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50013626 0.49986374] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.48749173 0.5125083 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.5010623 0.4989377] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5014884  0.49851164] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.49636188 0.50363815] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5017116  0.49828836] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.50054514 0.49945486] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49748304 0.502517  ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.4958392 0.5041608] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50199604 0.49800396] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.4912001  0.50879985] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.5010158  0.49898425] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49278465 0.5072154 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.4961734 0.5038266] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50229514 0.49770483] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5019361 0.4980639] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49497393 0.5050261 ] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.50047684 0.49952316] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5020445  0.49795556] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5017589 0.4982411] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.501614   0.49838606] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49860722 0.5013928 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.49766296 0.502337  ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.4965974 0.5034026] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.49928278 0.5007172 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50121385 0.49878612] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49607277 0.5039272 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5020524  0.49794754] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49919802 0.500802  ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5016977  0.49830228] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.49644855 0.5035515 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.50223833 0.49776164] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.50173295 0.49826705] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.50159496 0.49840507] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49222127 0.50777876] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.50086606 0.4991339 ] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.50043863 0.49956137] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.50221616 0.49778387] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.49822673 0.5017733 ] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5016329 0.4983671] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 56.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bb5ba",
   "metadata": {},
   "source": [
    "This model architecture took significantly more system resources to train.  After 10 epochs of traininig, the validation accuracy was still climbing but the loss had remained constsant.  This accuracy was still below 50% which leads me to believe that the model was not actually learning but moving towards predicting only on class.  When analyzing the `evaluate_model`, the confidence of the predictions is very close to a coin flip despite the model predicting both classes.  Again, the ROC AUC was 54% which is a terrible score for this model.\n",
    "\n",
    "After a further 10 epochs, the model reached a validation accuracy over 50%, 52.97%.  However, when evaluating the model and the test dataset, which it had not seen, the accuracy remained underwhelming at 56%.  Still, the ROC AUC remained well below an acceptable level with a mere 54.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "06ac562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm(x_input, outClasses):\n",
    "    visible = tf.keras.layers.Input(shape=(x_input.shape[1]))    \n",
    "    reshape = tf.keras.layers.Reshape(target_shape=(time_steps,features))(visible)    \n",
    "    \n",
    "    rnn = tf.compat.v1.keras.layers.LSTM(4, activation='relu', kernel_constraint=tf.keras.constraints.max_norm(3), return_sequences=True)(reshape)\n",
    "    rnn = tf.compat.v1.keras.layers.LSTM(4, activation='relu', kernel_constraint=tf.keras.constraints.max_norm(2))(rnn)\n",
    "    #rnn = Flatten()(rnn)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(outClasses, activation='softmax')(rnn)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "237b5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stacked_lstm(X_train, num_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a86de602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 15)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_32 (Reshape)         (None, 5, 3)              0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 5, 4)              128       \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 282\n",
      "Trainable params: 282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a6493627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw prediction: [0.4937 0.5063] predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# test a prediction to verify model design\n",
    "x_input = X_test[0]\n",
    "x_input = x_input.reshape(1,X_test.shape[1])\n",
    "yhat = model.predict(x_input)\n",
    "print(f'raw prediction: [{yhat[0][0]:.4} {yhat[0][1]:.4}] predicted label: {np.argmax(yhat[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7711557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1460/1460 - 116s - loss: 0.9970 - accuracy: 0.3227 - val_loss: 0.6948 - val_accuracy: 0.3367\n",
      "Epoch 2/10\n",
      "1460/1460 - 115s - loss: 0.9970 - accuracy: 0.3527 - val_loss: 0.6947 - val_accuracy: 0.3503\n",
      "Epoch 3/10\n",
      "1460/1460 - 117s - loss: 0.9970 - accuracy: 0.3761 - val_loss: 0.6947 - val_accuracy: 0.3656\n",
      "Epoch 4/10\n",
      "1460/1460 - 113s - loss: 0.9969 - accuracy: 0.3836 - val_loss: 0.6946 - val_accuracy: 0.4003\n",
      "Epoch 5/10\n",
      "1460/1460 - 114s - loss: 0.9969 - accuracy: 0.4260 - val_loss: 0.6946 - val_accuracy: 0.4484\n",
      "Epoch 6/10\n",
      "1460/1460 - 114s - loss: 0.9969 - accuracy: 0.4542 - val_loss: 0.6946 - val_accuracy: 0.4780\n",
      "Epoch 7/10\n",
      "1460/1460 - 114s - loss: 0.9968 - accuracy: 0.4742 - val_loss: 0.6946 - val_accuracy: 0.4912\n",
      "Epoch 8/10\n",
      "1460/1460 - 111s - loss: 0.9968 - accuracy: 0.4949 - val_loss: 0.6946 - val_accuracy: 0.4981\n",
      "Epoch 9/10\n",
      "1460/1460 - 116s - loss: 0.9968 - accuracy: 0.5005 - val_loss: 0.6946 - val_accuracy: 0.5039\n",
      "Epoch 10/10\n",
      "1460/1460 - 116s - loss: 0.9968 - accuracy: 0.5026 - val_loss: 0.6946 - val_accuracy: 0.5066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x260be1ffb80>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2, class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "56dfd0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 54.080%\n",
      "pred: [0.494 0.506] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.498 0.502] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.474 0.526] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.5 0.5] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.497 0.503] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.498 0.502] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.492 0.508] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.497 0.503] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.498 0.502] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.498 0.502] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.478 0.522] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.502 0.498] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.501 0.499] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.497 0.503] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.499 0.501] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.489 0.511] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.496 0.504] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.502 0.498] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.482 0.518] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.494 0.506] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.485 0.515] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.478 0.522] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.5 0.5] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.502 0.498] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.502 0.498] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.495 0.505] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.492 0.508] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.497 0.503] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.497 0.503] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.499 0.501] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.495 0.505] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "pred: [0.492 0.508] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.501 0.499] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.502 0.498] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.5 0.5] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.479 0.521] actual: [0. 1.] |||        pred: 1 actual: 1\n",
      "pred: [0.5 0.5] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.498 0.502] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.502 0.498] actual: [1. 0.] |||        pred: 0 actual: 0\n",
      "pred: [0.494 0.506] actual: [1. 0.] |||        pred: 1 actual: 0\n",
      "pred: [0.502 0.498] actual: [0. 1.] |||        pred: 0 actual: 1\n",
      "correct: 50.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf478d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "More of the same despite yet another model architecture.  ROC AUC of 54% and test dataset accuracy of 50%.  This leads me to believe that the dataset does not contain predictive features.\n",
    "\n",
    "To test this theory, I used another known dataset, the sonar dataset available here: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks), actually it's up one level so that link is the dataset description.  All of the models performed significantly better in every case.  Most achieved ROC AUC of well over 70%-80% which is a significant difference.\n",
    "\n",
    "This was a very interesting experiment in which I learned a ton.  I consider these models a basic framework on which to test additional filter style deep learning models for trading.  Going forward, I intend to investigate:\n",
    "\n",
    " - Additional indicators.\n",
    " - Larger window sizes.\n",
    " - Multi-headed models which can handle different types of data (indicator head, market opening type head, market state [consolidation expansion] head).\n",
    " - Additional model architectures such as Attention, Inception, ResNet, etc.\n",
    " - Level 2 order book data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedd7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
